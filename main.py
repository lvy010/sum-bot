#!/usr/bin/env python3
"""
CSDNåšå®¢ä½œå“é›†ç”Ÿæˆå™¨
è‡ªåŠ¨çˆ¬å–CSDNåšå®¢æ–‡ç« ï¼Œç”ŸæˆAIæœˆåº¦æ€»ç»“ï¼Œå¹¶åˆ›å»ºç²¾ç¾çš„ä½œå“é›†ç½‘ç«™
"""

import argparse
import json
import os
import sys
from datetime import datetime

from csdn_scraper import CSDNScraper
from smart_scraper import SmartCSDNScraper
from ai_summarizer import AISummarizer
from portfolio_generator import PortfolioGenerator
from config import Config

class CSDBlogPortfolio:
    def __init__(self):
        self.config = Config()
        self.scraper = CSDNScraper()
        self.smart_scraper = SmartCSDNScraper()
        self.summarizer = AISummarizer()
        self.generator = PortfolioGenerator()
        
    def scrape_articles(self, max_pages=None, force_refresh=False):
        """çˆ¬å–æ–‡ç« """
        articles_file = 'articles.json'
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®ä¸”ä¸å¼ºåˆ¶åˆ·æ–°
        if os.path.exists(articles_file) and not force_refresh:
            print("å‘ç°å·²æœ‰æ–‡ç« æ•°æ®ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®...")
            print("å¦‚éœ€é‡æ–°çˆ¬å–ï¼Œè¯·ä½¿ç”¨ --force-refresh å‚æ•°")
            with open(articles_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        print("å¼€å§‹çˆ¬å–CSDNåšå®¢æ–‡ç« ...")
        
        # ä¼˜å…ˆä½¿ç”¨æ™ºèƒ½çˆ¬è™«
        try:
            print("ä½¿ç”¨æ™ºèƒ½çˆ¬è™«ï¼ˆæ¨èï¼‰...")
            articles = self.smart_scraper.get_article_list_smart(max_pages=max_pages or 5)
            
            # è·å–æ–‡ç« å†…å®¹
            if articles:
                print(f"è·å–åˆ°{len(articles)}ç¯‡æ–‡ç« ï¼Œå¼€å§‹è·å–è¯¦ç»†å†…å®¹...")
                for i, article in enumerate(articles[:10], 1):  # é™åˆ¶è·å–å†…å®¹çš„æ–‡ç« æ•°é‡
                    print(f"è·å–ç¬¬{i}/{min(10, len(articles))}ç¯‡æ–‡ç« å†…å®¹...")
                    content = self.smart_scraper.get_article_content_smart(article['url'])
                    article['content'] = content
                    if i >= 10:  # æœ€å¤šè·å–10ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹
                        print("å·²è·å–å‰10ç¯‡æ–‡ç« çš„è¯¦ç»†å†…å®¹ï¼Œå…¶ä½™æ–‡ç« åªä¿ç•™åŸºæœ¬ä¿¡æ¯")
                        break
        except Exception as e:
            print(f"æ™ºèƒ½çˆ¬è™«å¤±è´¥: {e}")
            print("å›é€€åˆ°æ™®é€šçˆ¬è™«...")
            articles = self.scraper.scrape_all_articles(max_pages=max_pages)
        
        # ä¿å­˜æ–‡ç« æ•°æ®
        self.scraper.save_articles_to_json(articles, articles_file)
        
        return articles
    
    def generate_summaries(self, articles, force_refresh=False):
        """ç”Ÿæˆæœˆåº¦æ€»ç»“"""
        summaries_file = 'monthly_summaries.json'
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ€»ç»“ä¸”ä¸å¼ºåˆ¶åˆ·æ–°
        if os.path.exists(summaries_file) and not force_refresh:
            print("å‘ç°å·²æœ‰æœˆåº¦æ€»ç»“ï¼Œä½¿ç”¨ç¼“å­˜æ•°æ®...")
            print("å¦‚éœ€é‡æ–°ç”Ÿæˆï¼Œè¯·ä½¿ç”¨ --force-refresh å‚æ•°")
            with open(summaries_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        print("å¼€å§‹ç”ŸæˆAIæœˆåº¦æ€»ç»“...")
        summaries = self.summarizer.generate_all_monthly_summaries(articles)
        
        # ä¿å­˜æ€»ç»“æ•°æ®
        with open(summaries_file, 'w', encoding='utf-8') as f:
            json.dump(summaries, f, ensure_ascii=False, indent=2)
        
        print(f"æœˆåº¦æ€»ç»“å·²ä¿å­˜åˆ° {summaries_file}")
        return summaries
    
    def generate_portfolio(self, articles, summaries):
        """ç”Ÿæˆä½œå“é›†ç½‘ç«™"""
        print("å¼€å§‹ç”Ÿæˆä½œå“é›†ç½‘ç«™...")
        output_dir = self.generator.generate_portfolio(articles, summaries)
        return output_dir
    
    def run_full_pipeline(self, max_pages=None, force_refresh=False):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        try:
            print("=" * 60)
            print("CSDNåšå®¢ä½œå“é›†ç”Ÿæˆå™¨")
            print("=" * 60)
            
            # æ­¥éª¤1: çˆ¬å–æ–‡ç« 
            print("\nğŸ“„ æ­¥éª¤1: çˆ¬å–åšå®¢æ–‡ç« ")
            articles = self.scrape_articles(max_pages, force_refresh)
            print(f"âœ… æˆåŠŸè·å– {len(articles)} ç¯‡æ–‡ç« ")
            
            # æ­¥éª¤2: ç”Ÿæˆæœˆåº¦æ€»ç»“
            print("\nğŸ¤– æ­¥éª¤2: ç”ŸæˆAIæœˆåº¦æ€»ç»“")
            summaries = self.generate_summaries(articles, force_refresh)
            print(f"âœ… æˆåŠŸç”Ÿæˆ {len(summaries)} ä¸ªæœˆä»½çš„æ€»ç»“")
            
            # æ­¥éª¤3: ç”Ÿæˆä½œå“é›†ç½‘ç«™
            print("\nğŸŒ æ­¥éª¤3: ç”Ÿæˆä½œå“é›†ç½‘ç«™")
            output_dir = self.generate_portfolio(articles, summaries)
            print(f"âœ… ä½œå“é›†ç½‘ç«™å·²ç”Ÿæˆåˆ°: {output_dir}")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self._show_statistics(articles, summaries)
            
            # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜
            self._show_usage_instructions(output_dir)
            
            return True
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _show_statistics(self, articles, summaries):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        total_reads = sum(article.get('read_count', 0) for article in articles)
        total_likes = sum(article.get('like_count', 0) for article in articles)
        total_comments = sum(article.get('comment_count', 0) for article in articles)
        
        print("\nğŸ“Š åšå®¢ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   â€¢ æ€»æ–‡ç« æ•°: {len(articles)}")
        print(f"   â€¢ æ€»é˜…è¯»é‡: {total_reads:,}")
        print(f"   â€¢ æ€»ç‚¹èµæ•°: {total_likes:,}")
        print(f"   â€¢ æ€»è¯„è®ºæ•°: {total_comments:,}")
        print(f"   â€¢ æ´»è·ƒæœˆä»½: {len(summaries)}")
        
        if articles:
            avg_reads = total_reads // len(articles)
            print(f"   â€¢ å¹³å‡é˜…è¯»é‡: {avg_reads}")
    
    def _show_usage_instructions(self, output_dir):
        """æ˜¾ç¤ºä½¿ç”¨è¯´æ˜"""
        print("\nğŸ‰ ä½œå“é›†ç”Ÿæˆå®Œæˆï¼")
        print("\nğŸ“ ä½¿ç”¨è¯´æ˜:")
        print(f"   1. æ‰“å¼€æ–‡ä»¶å¤¹: {os.path.abspath(output_dir)}")
        print("   2. åŒå‡» index.html åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹")
        print("   3. æˆ–ä½¿ç”¨æœ¬åœ°æœåŠ¡å™¨:")
        print(f"      cd {output_dir}")
        print("      python -m http.server 8000")
        print("      ç„¶åè®¿é—®: http://localhost:8000")
        
        print("\nğŸ”„ æ›´æ–°è¯´æ˜:")
        print("   â€¢ è¦æ›´æ–°æ•°æ®ï¼Œè¯·ä½¿ç”¨ --force-refresh å‚æ•°é‡æ–°è¿è¡Œ")
        print("   â€¢ è¦åªæ›´æ–°æ€»ç»“ï¼Œè¯·åˆ é™¤ monthly_summaries.json åé‡æ–°è¿è¡Œ")
        
        print(f"\nâš™ï¸ é…ç½®æ–‡ä»¶: config.py")
        print(f"   â€¢ å¦‚éœ€ä½¿ç”¨AIæ€»ç»“åŠŸèƒ½ï¼Œè¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY")

def main():
    parser = argparse.ArgumentParser(
        description="CSDNåšå®¢ä½œå“é›†ç”Ÿæˆå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s                          # ç”Ÿæˆå®Œæ•´ä½œå“é›†
  %(prog)s --max-pages 10           # åªçˆ¬å–å‰10é¡µæ–‡ç« 
  %(prog)s --force-refresh          # å¼ºåˆ¶é‡æ–°çˆ¬å–å’Œç”Ÿæˆ
  %(prog)s --scrape-only            # åªçˆ¬å–æ–‡ç« ï¼Œä¸ç”Ÿæˆç½‘ç«™
  %(prog)s --generate-only          # åªç”Ÿæˆç½‘ç«™ï¼ˆéœ€è¦å·²æœ‰æ•°æ®ï¼‰
        """
    )
    
    parser.add_argument(
        '--max-pages', 
        type=int, 
        help='æœ€å¤§çˆ¬å–é¡µæ•°ï¼ˆé»˜è®¤çˆ¬å–æ‰€æœ‰é¡µé¢ï¼‰'
    )
    
    parser.add_argument(
        '--force-refresh', 
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°çˆ¬å–æ–‡ç« å’Œç”Ÿæˆæ€»ç»“'
    )
    
    parser.add_argument(
        '--scrape-only', 
        action='store_true',
        help='åªçˆ¬å–æ–‡ç« ï¼Œä¸ç”Ÿæˆç½‘ç«™å’Œæ€»ç»“'
    )
    
    parser.add_argument(
        '--generate-only', 
        action='store_true',
        help='åªç”Ÿæˆç½‘ç«™ï¼Œä½¿ç”¨å·²æœ‰çš„æ–‡ç« å’Œæ€»ç»“æ•°æ®'
    )
    
    parser.add_argument(
        '--no-ai', 
        action='store_true',
        help='ä¸ä½¿ç”¨AIç”Ÿæˆæ€»ç»“ï¼Œåªç”ŸæˆåŸºç¡€ç»Ÿè®¡'
    )
    
    parser.add_argument(
        '--use-smart-scraper', 
        action='store_true',
        help='ä½¿ç”¨æ™ºèƒ½çˆ¬è™«ï¼ˆæ¨èï¼Œèƒ½æ›´å¥½åœ°å¤„ç†åçˆ¬è™«æœºåˆ¶ï¼‰'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸»åº”ç”¨å®ä¾‹
    app = CSDBlogPortfolio()
    
    try:
        if args.scrape_only:
            # åªçˆ¬å–æ–‡ç« 
            articles = app.scrape_articles(args.max_pages, args.force_refresh)
            print(f"âœ… æ–‡ç« çˆ¬å–å®Œæˆï¼Œå…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
            
        elif args.generate_only:
            # åªç”Ÿæˆç½‘ç«™
            if not os.path.exists('articles.json'):
                print("âŒ æœªæ‰¾åˆ°æ–‡ç« æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œçˆ¬è™«")
                return 1
            
            with open('articles.json', 'r', encoding='utf-8') as f:
                articles = json.load(f)
            
            summaries = {}
            if os.path.exists('monthly_summaries.json'):
                with open('monthly_summaries.json', 'r', encoding='utf-8') as f:
                    summaries = json.load(f)
            
            app.generate_portfolio(articles, summaries)
            print("âœ… ä½œå“é›†ç½‘ç«™ç”Ÿæˆå®Œæˆ")
            
        else:
            # è¿è¡Œå®Œæ•´æµç¨‹
            success = app.run_full_pipeline(args.max_pages, args.force_refresh)
            return 0 if success else 1
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())
