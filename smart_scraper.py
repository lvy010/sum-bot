#!/usr/bin/env python3
"""
æ™ºèƒ½çˆ¬è™« - ä¸“é—¨å¤„ç†CSDNåçˆ¬è™«æœºåˆ¶
åŒ…å«å¤šç§ååçˆ¬ç­–ç•¥ï¼šä»£ç†è½®æ¢ã€è¯·æ±‚å¤´è½®æ¢ã€æ™ºèƒ½å»¶è¿Ÿç­‰
"""

import requests
import time
import random
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from config import Config

class SmartCSDNScraper:
    def __init__(self):
        self.config = Config()
        self.session = requests.Session()
        self.setup_session()
        
        # ç”¨æˆ·ä»£ç†æ± 
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2.1 Safari/605.1.15'
        ]
        
    def setup_session(self):
        """è®¾ç½®ä¼šè¯"""
        # è®¾ç½®é‡è¯•ç­–ç•¥
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        retry_strategy = Retry(
            total=5,
            backoff_factor=3,
            status_forcelist=[429, 500, 502, 503, 504, 521, 522, 524],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # è®¾ç½®è¿æ¥æ± 
        self.session.mount('https://', HTTPAdapter(pool_connections=1, pool_maxsize=1))
    
    def get_random_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        }
    
    def smart_delay(self, base_delay=3):
        """æ™ºèƒ½å»¶è¿Ÿ - æ¨¡æ‹Ÿäººç±»è¡Œä¸º"""
        # åŸºç¡€å»¶è¿Ÿ + éšæœºå»¶è¿Ÿ
        delay = base_delay + random.uniform(1, 4)
        
        # å¶å°”æ·»åŠ æ›´é•¿çš„å»¶è¿Ÿï¼Œæ¨¡æ‹Ÿç”¨æˆ·é˜…è¯»æ—¶é—´
        if random.random() < 0.1:  # 10%æ¦‚ç‡
            delay += random.uniform(5, 15)
            print(f"æ¨¡æ‹Ÿç”¨æˆ·é˜…è¯»ï¼Œé¢å¤–ç­‰å¾…{delay-base_delay:.1f}ç§’...")
        
        time.sleep(delay)
    
    def safe_request(self, url, max_retries=5):
        """å®‰å…¨è¯·æ±‚ - å¤„ç†521ç­‰é”™è¯¯"""
        for attempt in range(max_retries):
            try:
                # æ¯æ¬¡è¯·æ±‚ä½¿ç”¨éšæœºè¯·æ±‚å¤´
                headers = self.get_random_headers()
                
                # æ·»åŠ Referer
                if attempt > 0:
                    headers['Referer'] = self.config.CSDN_BASE_URL
                
                print(f"è¯·æ±‚ {url} (å°è¯• {attempt + 1}/{max_retries})")
                
                response = self.session.get(
                    url, 
                    headers=headers, 
                    timeout=30,
                    allow_redirects=True
                )
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code == 200:
                    return response
                elif response.status_code == 521:
                    wait_time = (attempt + 1) * 10 + random.uniform(5, 15)
                    print(f"521é”™è¯¯ï¼Œç­‰å¾…{wait_time:.1f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                elif response.status_code == 403:
                    wait_time = (attempt + 1) * 15 + random.uniform(10, 20)
                    print(f"403é”™è¯¯ï¼Œç­‰å¾…{wait_time:.1f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    print(f"HTTP {response.status_code}: {response.reason}")
                    if attempt < max_retries - 1:
                        self.smart_delay(5)
                        continue
                    else:
                        break
                        
            except requests.exceptions.RequestException as e:
                print(f"è¯·æ±‚å¼‚å¸¸: {e}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    print(f"ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                else:
                    break
        
        return None
    
    def get_article_list_smart(self, max_pages=3):
        """æ™ºèƒ½è·å–æ–‡ç« åˆ—è¡¨"""
        articles = []
        
        print(f"å¼€å§‹æ™ºèƒ½çˆ¬å–ï¼Œæœ€å¤š{max_pages}é¡µ...")
        
        for page in range(1, max_pages + 1):
            print(f"\n=== çˆ¬å–ç¬¬{page}é¡µ ===")
            
            # æ„å»ºURL
            url = f"{self.config.CSDN_BASE_URL}/article/list/{page}"
            
            # æ™ºèƒ½å»¶è¿Ÿ
            if page > 1:
                self.smart_delay()
            
            # å®‰å…¨è¯·æ±‚
            response = self.safe_request(url)
            
            if not response:
                print(f"ç¬¬{page}é¡µè¯·æ±‚å¤±è´¥ï¼Œè·³è¿‡")
                continue
            
            # è§£æé¡µé¢
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ–‡ç«  - å¤šç§é€‰æ‹©å™¨
            selectors = [
                'div.article-item-box',
                'div.blog-list-box', 
                'article',
                'div[class*="article"]',
                'div[class*="blog"]'
            ]
            
            article_items = []
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    article_items = items
                    print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ°{len(items)}ç¯‡æ–‡ç« ")
                    break
            
            if not article_items:
                print("æœªæ‰¾åˆ°æ–‡ç« ï¼Œå¯èƒ½é‡åˆ°åçˆ¬é™åˆ¶")
                print("é¡µé¢å†…å®¹é¢„è§ˆ:")
                print(response.text[:500])
                continue
            
            # æå–æ–‡ç« ä¿¡æ¯
            page_articles = []
            for item in article_items:
                article_info = self.extract_article_info_smart(item)
                if article_info:
                    page_articles.append(article_info)
            
            articles.extend(page_articles)
            print(f"ç¬¬{page}é¡µæˆåŠŸè·å–{len(page_articles)}ç¯‡æ–‡ç« ")
            
            # å¦‚æœè·å–çš„æ–‡ç« å¾ˆå°‘ï¼Œå¯èƒ½é‡åˆ°äº†é™åˆ¶
            if len(page_articles) < 5 and page > 1:
                print("è·å–æ–‡ç« æ•°é‡å¼‚å¸¸ï¼Œå¯èƒ½é‡åˆ°åçˆ¬é™åˆ¶ï¼Œåœæ­¢çˆ¬å–")
                break
        
        return articles
    
    def extract_article_info_smart(self, item):
        """æ™ºèƒ½æå–æ–‡ç« ä¿¡æ¯"""
        try:
            # å¤šç§æ–¹å¼æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
            title_elem = (
                item.select_one('h4 a') or
                item.find('a', class_='title') or
                item.find('h4') or
                item.find('a')
            )
            
            if not title_elem:
                return None
            
            title = title_elem.get_text(strip=True)
            
            # è·å–é“¾æ¥ï¼Œä¼˜å…ˆä»aæ ‡ç­¾è·å–
            if title_elem.name == 'a':
                article_url = title_elem.get('href', '')
            else:
                # å¦‚æœæ˜¯h4ç­‰å…¶ä»–æ ‡ç­¾ï¼ŒæŸ¥æ‰¾å†…éƒ¨çš„aæ ‡ç­¾
                link_elem = title_elem.find('a')
                article_url = link_elem.get('href', '') if link_elem else ''
            
            if not article_url.startswith('http'):
                article_url = urljoin(self.config.CSDN_BASE_URL, article_url)
            
            # æå–æ—¶é—´
            time_elem = item.find('span', class_='date') or item.find('time')
            publish_time = time_elem.get_text(strip=True) if time_elem else ""
            
            # æå–ç»Ÿè®¡æ•°æ®
            read_count = self.extract_number(item, ['é˜…è¯»', 'read'])
            like_count = self.extract_number(item, ['ç‚¹èµ', 'like', 'ğŸ‘'])
            comment_count = self.extract_number(item, ['è¯„è®º', 'comment'])
            
            return {
                'title': title,
                'url': article_url,
                'publish_time': publish_time,
                'read_count': read_count,
                'like_count': like_count,
                'comment_count': comment_count,
                'content': ''  # ç¨åè·å–
            }
            
        except Exception as e:
            print(f"æå–æ–‡ç« ä¿¡æ¯å‡ºé”™: {e}")
            return None
    
    def extract_number(self, element, keywords):
        """ä»å…ƒç´ ä¸­æå–æ•°å­—"""
        try:
            text = element.get_text()
            for keyword in keywords:
                if keyword in text:
                    import re
                    numbers = re.findall(r'\d+', text)
                    if numbers:
                        return int(numbers[0])
            return 0
        except:
            return 0
    
    def get_article_content_smart(self, article_url):
        """æ™ºèƒ½è·å–æ–‡ç« å†…å®¹"""
        print(f"è·å–æ–‡ç« å†…å®¹: {article_url}")
        
        # æ™ºèƒ½å»¶è¿Ÿ
        self.smart_delay(2)
        
        response = self.safe_request(article_url)
        if not response:
            return ""
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å¤šç§å†…å®¹é€‰æ‹©å™¨
        content_selectors = [
            '#content_views',
            '.markdown_views',
            '.htmledit_views', 
            'article',
            '.blog-content-box',
            '.article-content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator='\n', strip=True)
                # é™åˆ¶é•¿åº¦
                if len(content) > 1500:
                    content = content[:1500] + "..."
                return content
        
        return ""

if __name__ == "__main__":
    # æµ‹è¯•æ™ºèƒ½çˆ¬è™«
    scraper = SmartCSDNScraper()
    articles = scraper.get_article_list_smart(max_pages=2)
    
    print(f"\n=== çˆ¬å–ç»“æœ ===")
    print(f"å…±è·å– {len(articles)} ç¯‡æ–‡ç« ")
    
    for i, article in enumerate(articles[:3], 1):
        print(f"{i}. {article['title']}")
        print(f"   é˜…è¯»: {article['read_count']}, ç‚¹èµ: {article['like_count']}")
        print(f"   URL: {article['url']}")
        print()
